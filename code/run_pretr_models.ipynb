{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Wav2Vec2Model\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from collect_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 93680])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 292, 768]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dati Nostri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAVE FILES DURATION:  {'lw1': 318.1481632653061, 'lw1_3': 53.072426303854876, 'lw1_2': 93.7512925170068, 'lw1_1': 75.44276643990929, 'lw1_0': 95.88167800453515, 'easy_money_7': 129.30312925170068, 'easy_money_6': 123.42730158730159, 'easy_money_4': 163.68920634920636, 'easy_money_5': 142.36866213151927, 'easy_money_3': 141.97519274376418, 'easy_money_0': 143.12875283446712, 'easy_money_2': 180.43360544217688, 'easy_money_1': 104.10834467120182, 'cable_spool_fort_4': 123.26390022675737, 'cable_spool_fort_5': 54.69605442176871, 'cable_spool_fort_3': 105.82820861678005, 'cable_spool_fort_2': 137.65614512471655, 'cable_spool_fort_1': 134.8726984126984, 'cable_spool_fort_0': 100.59777777777778, 'the_black_willow_9': 188.23668934240362, 'the_black_willow_8': 142.4142403628118, 'the_black_willow_7': 172.399410430839, 'the_black_willow_6': 128.21628117913832, 'the_black_willow_5': 81.94331065759637, 'the_black_willow_4': 110.45659863945578, 'the_black_willow_2': 125.23859410430839, 'the_black_willow_3': 131.8818140589569, 'the_black_willow_10': 189.4689342403628, 'the_black_willow_11': 119.1855328798186, 'the_black_willow_1': 149.3205442176871, 'the_black_willow_0': 135.09065759637187}\n",
      "WAVE FILES WITH\\ NUMBERS:  {'lw1': 0.0, 'cable_spool_fort': 1.0, 'easy_money': 2.0, 'the_black_willow': 3.0}\n"
     ]
    }
   ],
   "source": [
    "stimuli_path = meg_path + '/stimuli/audio'\n",
    "wav_files_duration = {}\n",
    "\n",
    "for filename in os.listdir(stimuli_path):\n",
    "    if filename.endswith('.wav'): \n",
    "        file_path = os.path.join(stimuli_path, filename)\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        wav_files_duration[filename.rstrip('.wav')] = duration\n",
    "print('WAVE FILES DURATION: ',wav_files_duration)\n",
    "print('WAVE FILES WITH\\ NUMBERS: ',task)\n",
    "wav_list_without_numb = list(task.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATIENT:  01\n",
      "AUDIO_NAME:  lw1\n"
     ]
    }
   ],
   "source": [
    "audio_name = wav_list_without_numb[0]\n",
    "subject = patient[0]\n",
    "print('PATIENT: ', subject)\n",
    "print('AUDIO_NAME: ', audio_name)\n",
    "audio_input = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SESSION:  0\n",
      "STORY_UID_OR_TASK:  0\n",
      "Reading 0 ... 395999  =      0.000 ...   395.999 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteoc/spect-to-meg/code/collect_data.py:47: RuntimeWarning: The unit for channel(s) MISC 001, MISC 002, MISC 003, MISC 004, MISC 005, MISC 006, MISC 007, MISC 008, MISC 009, MISC 010, MISC 011, MISC 012, MISC 013, MISC 014, MISC 015, MISC 016, MISC 017, MISC 018, MISC 019, MISC 020, MISC 021, MISC 022, MISC 023, MISC 024, MISC 025, MISC 026, MISC 027, MISC 028, MISC 029, MISC 030, MISC 031, MISC 032 has changed from V to NA.\n",
      "  raw = mne_bids.read_raw_bids(bids_path, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 6601 samples (6.601 s)\n",
      "\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "SOUND_ID:  0.0\n",
      "Adding metadata with 18 columns\n",
      "180 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 180 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "AUDIO_SPECTR_SHAPE:  torch.Size([180, 48000])\n",
      "SOUND_ID:  1.0\n",
      "Adding metadata with 18 columns\n",
      "139 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 139 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "AUDIO_SPECTR_SHAPE:  torch.Size([139, 48000])\n",
      "SOUND_ID:  2.0\n",
      "Adding metadata with 18 columns\n",
      "225 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 225 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "AUDIO_SPECTR_SHAPE:  torch.Size([225, 48000])\n",
      "SOUND_ID:  3.0\n",
      "Adding metadata with 18 columns\n",
      "124 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 124 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "AUDIO_SPECTR_SHAPE:  torch.Size([124, 48000])\n",
      "SESSION:  1\n",
      "STORY_UID_OR_TASK:  0\n",
      "Reading 0 ... 459999  =      0.000 ...   459.999 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteoc/spect-to-meg/code/collect_data.py:47: RuntimeWarning: The unit for channel(s) MISC 001, MISC 002, MISC 003, MISC 004, MISC 005, MISC 006, MISC 007, MISC 008, MISC 009, MISC 010, MISC 011, MISC 012, MISC 013, MISC 014, MISC 015, MISC 016, MISC 017, MISC 018, MISC 019, MISC 020, MISC 021, MISC 022, MISC 023, MISC 024, MISC 025, MISC 026, MISC 027, MISC 028, MISC 029, MISC 030, MISC 031, MISC 032 has changed from V to NA.\n",
      "  raw = mne_bids.read_raw_bids(bids_path, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 6601 samples (6.601 s)\n",
      "\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "SOUND_ID:  0.0\n",
      "Adding metadata with 18 columns\n",
      "180 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 180 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "SOUND_ID:  1.0\n",
      "Adding metadata with 18 columns\n",
      "139 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 139 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "SOUND_ID:  2.0\n",
      "Adding metadata with 18 columns\n",
      "225 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 225 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "SOUND_ID:  3.0\n",
      "Adding metadata with 18 columns\n",
      "124 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 124 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(session)):\n",
    "    print(\"SESSION: \", session[i])\n",
    "    story_uid = int(task[audio_name])\n",
    "    print(\"STORY_UID_OR_TASK: \", story_uid)\n",
    "    raw = get_bids_raw(meg_path, subject, session[i], str(story_uid))\n",
    "    for z in range(len(lw1)):\n",
    "        print(\"SOUND_ID: \", float(lw1[z]))\n",
    "        epochs_data = get_epochs(raw, float(story_uid), float(lw1[z]))\n",
    "        if (i == 0 and subject == '01'):\n",
    "            audio_path = stimuli_path + '/' + audio_name + '_' + str(z) + '.wav'\n",
    "            data_audio_chunks = []\n",
    "            epoch_spectr = get_meg_from_raw_epochs(epochs_data)\n",
    "            for j in range(epoch_spectr.shape[0]):\n",
    "                start = epochs_data[j]._metadata[\"start\"].item()\n",
    "                duration = 3\n",
    "                y, sr = librosa.load(audio_path, sr=sampling_audio, offset=start, duration=duration)\n",
    "                if (y.shape[0] < duration*sampling_audio):   \n",
    "                    # make padding         \n",
    "                    pad_width = duration*sampling_audio - y.shape[0]\n",
    "                    y = np.pad(y, (0, pad_width), mode='constant', constant_values=0)\n",
    "                data_audio_chunks.append(y)\n",
    "            audio_tensor_chunk = torch.tensor(data_audio_chunks)\n",
    "            audio_input.append(audio_tensor_chunk)\n",
    "            print('AUDIO_SPECTR_SHAPE: ', audio_tensor_chunk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_input[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "inputs = processor(audio_input[1][0], sampling_rate=sampling_audio, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 48000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 149, 768]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-meg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
