{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteoc/miniconda3/envs/speech-meg/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Wav2Vec2Model\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from collect_data import *\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 93680])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 292, 768]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dati Nostri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAVE FILES DURATION:  {'lw1': 318.1481632653061, 'lw1_3': 53.072426303854876, 'lw1_2': 93.7512925170068, 'lw1_1': 75.44276643990929, 'lw1_0': 95.88167800453515, 'easy_money_7': 129.30312925170068, 'easy_money_6': 123.42730158730159, 'easy_money_4': 163.68920634920636, 'easy_money_5': 142.36866213151927, 'easy_money_3': 141.97519274376418, 'easy_money_0': 143.12875283446712, 'easy_money_2': 180.43360544217688, 'easy_money_1': 104.10834467120182, 'cable_spool_fort_4': 123.26390022675737, 'cable_spool_fort_5': 54.69605442176871, 'cable_spool_fort_3': 105.82820861678005, 'cable_spool_fort_2': 137.65614512471655, 'cable_spool_fort_1': 134.8726984126984, 'cable_spool_fort_0': 100.59777777777778, 'the_black_willow_9': 188.23668934240362, 'the_black_willow_8': 142.4142403628118, 'the_black_willow_7': 172.399410430839, 'the_black_willow_6': 128.21628117913832, 'the_black_willow_5': 81.94331065759637, 'the_black_willow_4': 110.45659863945578, 'the_black_willow_2': 125.23859410430839, 'the_black_willow_3': 131.8818140589569, 'the_black_willow_10': 189.4689342403628, 'the_black_willow_11': 119.1855328798186, 'the_black_willow_1': 149.3205442176871, 'the_black_willow_0': 135.09065759637187}\n",
      "WAVE FILES WITH\\ NUMBERS:  {'lw1': 0.0, 'cable_spool_fort': 1.0, 'easy_money': 2.0, 'the_black_willow': 3.0}\n"
     ]
    }
   ],
   "source": [
    "stimuli_path = meg_path + '/stimuli/audio'\n",
    "wav_files_duration = {}\n",
    "\n",
    "for filename in os.listdir(stimuli_path):\n",
    "    if filename.endswith('.wav'): \n",
    "        file_path = os.path.join(stimuli_path, filename)\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        wav_files_duration[filename.rstrip('.wav')] = duration\n",
    "print('WAVE FILES DURATION: ',wav_files_duration)\n",
    "print('WAVE FILES WITH\\ NUMBERS: ',task)\n",
    "wav_list_without_numb = list(task.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_input = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATIENT:  01\n",
      "AUDIO_NAME:  the_black_willow\n"
     ]
    }
   ],
   "source": [
    "audio_name = wav_list_without_numb[3]\n",
    "subject = patient[0]\n",
    "print('PATIENT: ', subject)\n",
    "print('AUDIO_NAME: ', audio_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SESSION:  0\n",
      "STORY_UID_OR_TASK:  3\n",
      "Reading 0 ... 1868999  =      0.000 ...  1868.999 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteoc/spect-to-meg/code/collect_data.py:47: RuntimeWarning: The unit for channel(s) MISC 001, MISC 002, MISC 003, MISC 004, MISC 005, MISC 006, MISC 007, MISC 008, MISC 009, MISC 010, MISC 011, MISC 012, MISC 013, MISC 014, MISC 015, MISC 016, MISC 017, MISC 018, MISC 019, MISC 020, MISC 021, MISC 022, MISC 023, MISC 024, MISC 025, MISC 026, MISC 027, MISC 028, MISC 029, MISC 030, MISC 031, MISC 032 has changed from V to NA.\n",
      "  raw = mne_bids.read_raw_bids(bids_path, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 6601 samples (6.601 s)\n",
      "\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "SOUND_ID:  0.0\n",
      "Adding metadata with 18 columns\n",
      "358 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 358 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "AUDIO_SPECTR_SHAPE:  torch.Size([358, 48000])\n",
      "SOUND_ID:  1.0\n",
      "Adding metadata with 18 columns\n",
      "262 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 262 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "AUDIO_SPECTR_SHAPE:  torch.Size([262, 48000])\n",
      "SOUND_ID:  2.0\n",
      "Adding metadata with 18 columns\n",
      "286 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 286 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "AUDIO_SPECTR_SHAPE:  torch.Size([286, 48000])\n",
      "SOUND_ID:  3.0\n",
      "Adding metadata with 18 columns\n",
      "306 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 306 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "AUDIO_SPECTR_SHAPE:  torch.Size([306, 48000])\n",
      "SOUND_ID:  4.0\n",
      "Adding metadata with 18 columns\n",
      "247 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 247 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "AUDIO_SPECTR_SHAPE:  torch.Size([247, 48000])\n",
      "SOUND_ID:  5.0\n",
      "Adding metadata with 18 columns\n",
      "185 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 185 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "AUDIO_SPECTR_SHAPE:  torch.Size([185, 48000])\n",
      "SOUND_ID:  6.0\n",
      "Adding metadata with 18 columns\n",
      "256 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 256 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "AUDIO_SPECTR_SHAPE:  torch.Size([256, 48000])\n",
      "SOUND_ID:  7.0\n",
      "Adding metadata with 18 columns\n",
      "384 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 384 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "AUDIO_SPECTR_SHAPE:  torch.Size([384, 48000])\n",
      "SOUND_ID:  8.0\n",
      "Adding metadata with 18 columns\n",
      "309 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 309 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "AUDIO_SPECTR_SHAPE:  torch.Size([309, 48000])\n",
      "SOUND_ID:  9.0\n",
      "Adding metadata with 18 columns\n",
      "411 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 411 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "AUDIO_SPECTR_SHAPE:  torch.Size([411, 48000])\n",
      "SOUND_ID:  10.0\n",
      "Adding metadata with 18 columns\n",
      "411 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 411 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "AUDIO_SPECTR_SHAPE:  torch.Size([411, 48000])\n",
      "SOUND_ID:  11.0\n",
      "Adding metadata with 18 columns\n",
      "338 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 338 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "AUDIO_SPECTR_SHAPE:  torch.Size([338, 48000])\n",
      "SESSION:  1\n",
      "STORY_UID_OR_TASK:  3\n",
      "Reading 0 ... 1771999  =      0.000 ...  1771.999 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteoc/spect-to-meg/code/collect_data.py:47: RuntimeWarning: The unit for channel(s) MISC 001, MISC 002, MISC 003, MISC 004, MISC 005, MISC 006, MISC 007, MISC 008, MISC 009, MISC 010, MISC 011, MISC 012, MISC 013, MISC 014, MISC 015, MISC 016, MISC 017, MISC 018, MISC 019, MISC 020, MISC 021, MISC 022, MISC 023, MISC 024, MISC 025, MISC 026, MISC 027, MISC 028, MISC 029, MISC 030, MISC 031, MISC 032 has changed from V to NA.\n",
      "  raw = mne_bids.read_raw_bids(bids_path, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 6601 samples (6.601 s)\n",
      "\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "SOUND_ID:  0.0\n",
      "Adding metadata with 18 columns\n",
      "358 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 358 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "SOUND_ID:  1.0\n",
      "Adding metadata with 18 columns\n",
      "262 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 262 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "SOUND_ID:  2.0\n",
      "Adding metadata with 18 columns\n",
      "286 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 286 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "SOUND_ID:  3.0\n",
      "Adding metadata with 18 columns\n",
      "306 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 306 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "SOUND_ID:  4.0\n",
      "Adding metadata with 18 columns\n",
      "247 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 247 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "SOUND_ID:  5.0\n",
      "Adding metadata with 18 columns\n",
      "185 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 185 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "SOUND_ID:  6.0\n",
      "Adding metadata with 18 columns\n",
      "256 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 256 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "SOUND_ID:  7.0\n",
      "Adding metadata with 18 columns\n",
      "384 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 384 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "SOUND_ID:  8.0\n",
      "Adding metadata with 18 columns\n",
      "309 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 309 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "SOUND_ID:  9.0\n",
      "Adding metadata with 18 columns\n",
      "411 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 411 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "SOUND_ID:  10.0\n",
      "Adding metadata with 18 columns\n",
      "411 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 411 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "SOUND_ID:  11.0\n",
      "Adding metadata with 18 columns\n",
      "338 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 338 events and 3201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(session)):\n",
    "    print(\"SESSION: \", session[i])\n",
    "    story_uid = int(task[audio_name])\n",
    "    print(\"STORY_UID_OR_TASK: \", story_uid)\n",
    "    raw = get_bids_raw(meg_path, subject, session[i], str(story_uid))\n",
    "    for z in range(len(the_black_willow)):\n",
    "        print(\"SOUND_ID: \", float(the_black_willow[z]))\n",
    "        epochs_data = get_epochs(raw, float(story_uid), float(the_black_willow[z]))\n",
    "        if (i == 0 and subject == '01'):\n",
    "            audio_path = stimuli_path + '/' + audio_name + '_' + str(z) + '.wav'\n",
    "            data_audio_chunks = []\n",
    "            epoch_spectr = get_meg_from_raw_epochs(epochs_data)\n",
    "            for j in range(epoch_spectr.shape[0]):\n",
    "                start = epochs_data[j]._metadata[\"start\"].item()\n",
    "                duration = 3\n",
    "                y, sr = librosa.load(audio_path, sr=sampling_audio, offset=start, duration=duration)\n",
    "                if (y.shape[0] < duration*sampling_audio):   \n",
    "                    # make padding         \n",
    "                    pad_width = duration*sampling_audio - y.shape[0]\n",
    "                    y = np.pad(y, (0, pad_width), mode='constant', constant_values=0)\n",
    "                data_audio_chunks.append(y)\n",
    "            audio_tensor_chunk = torch.tensor(data_audio_chunks)\n",
    "            audio_input.append(audio_tensor_chunk)\n",
    "            print('AUDIO_SPECTR_SHAPE: ', audio_tensor_chunk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(audio_input))\n",
    "# combined_tensor = torch.cat(audio_input, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION_AUDIO_TENSOR_TRAIN:  torch.Size([11958, 48000])\n",
      "DIMENSION_AUDIO_TENSOR_VALID:  torch.Size([1684, 48000])\n",
      "DIMENSION_AUDIO_TENSOR_TEST:  torch.Size([3480, 48000])\n"
     ]
    }
   ],
   "source": [
    "tensor_list_train = []\n",
    "tensor_list_valid = []\n",
    "tensor_list_test = []\n",
    "for file_tensor in audio_input:\n",
    "    train_tensor, val_tensor, test_tensor = split_tensor(file_tensor)\n",
    "    tensor_list_train.append(train_tensor)\n",
    "    tensor_list_valid.append(val_tensor)\n",
    "    tensor_list_test.append(test_tensor)\n",
    "audio_tensor_train = torch.cat(tensor_list_train, dim=0)\n",
    "audio_tensor_valid = torch.cat(tensor_list_valid, dim=0)\n",
    "audio_tensor_test = torch.cat(tensor_list_test, dim=0)\n",
    "\n",
    "audio_tensor_train = torch.cat((audio_tensor_train, audio_tensor_train), 0)\n",
    "audio_tensor_valid = torch.cat((audio_tensor_valid, audio_tensor_valid), 0)\n",
    "audio_tensor_test = torch.cat((audio_tensor_test, audio_tensor_test), 0)\n",
    "print('DIMENSION_AUDIO_TENSOR_TRAIN: ', audio_tensor_train.shape)\n",
    "print('DIMENSION_AUDIO_TENSOR_VALID: ', audio_tensor_valid.shape)\n",
    "print('DIMENSION_AUDIO_TENSOR_TEST: ', audio_tensor_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "inputs = processor(audio_tensor_test, sampling_rate=sampling_audio, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_input_values = inputs.input_values.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3480, 149, 768]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(w2v_input_values)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_pred_target_test = os.path.join(meg_path, 'collect_data/wave2vec_input_test.pt')\n",
    "# torch.save(last_hidden_states, save_pred_target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11958, 149, 768])\n",
      "torch.Size([3480, 149, 768])\n"
     ]
    }
   ],
   "source": [
    "w2v_input_tensor = torch.load(os.path.join(meg_path, 'collect_data/wave2vec_input.pt'))\n",
    "w2v_input_tensor_test = torch.load(os.path.join(meg_path, 'collect_data/wave2vec_input_test.pt'))\n",
    "print(w2v_input_tensor.shape)\n",
    "print(w2v_input_tensor_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION_MEG_TENSOR_TRAIN:  torch.Size([11958, 208, 16, 26])\n",
      "DIMENSION_MEG_TENSOR_VALID:  torch.Size([1684, 208, 16, 26])\n",
      "DIMENSION_MEG_TENSOR_TEST:  torch.Size([3480, 208, 16, 26])\n"
     ]
    }
   ],
   "source": [
    "megsp_path = os.path.join(meg_path, 'collect_data/megsp')\n",
    "megsp_list = os.listdir(megsp_path)\n",
    "\n",
    "megsp_list_session_0 = [f for f in megsp_list if f.split('_')[1] == '0']\n",
    "megsp_list_session_1 = [f for f in megsp_list if f.split('_')[1] == '1']\n",
    "\n",
    "meg_0_tensor_train, meg_0_tensor_valid, meg_0_tensor_test = get_splitted_tensor(megsp_list_session_0, megsp_path)\n",
    "meg_1_tensor_train, meg_1_tensor_valid, meg_1_tensor_test = get_splitted_tensor(megsp_list_session_1, megsp_path)\n",
    "meg_tensor_train = torch.cat((meg_0_tensor_train, meg_1_tensor_train), 0)\n",
    "meg_tensor_valid = torch.cat((meg_0_tensor_valid, meg_1_tensor_valid), 0)\n",
    "meg_tensor_test = torch.cat((meg_0_tensor_test, meg_1_tensor_test), 0)\n",
    "print('DIMENSION_MEG_TENSOR_TRAIN: ', meg_tensor_train.shape)\n",
    "print('DIMENSION_MEG_TENSOR_VALID: ', meg_tensor_valid.shape)\n",
    "print('DIMENSION_MEG_TENSOR_TEST: ', meg_tensor_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [4:27:51<00:00, 77.27s/it]  \n"
     ]
    }
   ],
   "source": [
    "pred_target = []\n",
    "mse_scores = []\n",
    "real_target = []\n",
    "audio_train = w2v_input_tensor.reshape(w2v_input_tensor.shape[0], -1)\n",
    "audio_test = w2v_input_tensor_test.reshape(w2v_input_tensor_test.shape[0], -1)\n",
    "\n",
    "for channel in tqdm(range(num_channel)):   \n",
    "    y_train = meg_tensor_train[:, channel, :, :].reshape(meg_tensor_train.shape[0], -1)\n",
    "    y_test = meg_tensor_test[:, channel, :, :].reshape(meg_tensor_test.shape[0], -1)\n",
    "\n",
    "    model = Ridge(alpha=5000, max_iter=1000)\n",
    "    model.fit(audio_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(audio_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    pred_target.append(y_pred)\n",
    "    real_target.append(y_test)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_306439/492901017.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /croot/pytorch_1686931851744/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  torch.save(torch.tensor(pred_target), save_pred_target)\n"
     ]
    }
   ],
   "source": [
    "save_pred_target = os.path.join(meg_path, 'collect_data/meg_prediction_ridge_w2v.pt')\n",
    "torch.save(torch.tensor(pred_target), save_pred_target)\n",
    "save_mse = os.path.join(meg_path, 'collect_data/meg_mse_ridge_w2v.pt')\n",
    "torch.save(torch.tensor(mse_scores), save_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-meg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
