{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collect_data import *\n",
    "from collect_metrics import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.linear_model import Ridge\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAVE FILES DURATION:  {'lw1': 318.1481632653061, 'lw1_3': 53.072426303854876, 'lw1_2': 93.7512925170068, 'lw1_1': 75.44276643990929, 'lw1_0': 95.88167800453515, 'easy_money_7': 129.30312925170068, 'easy_money_6': 123.42730158730159, 'easy_money_4': 163.68920634920636, 'easy_money_5': 142.36866213151927, 'easy_money_3': 141.97519274376418, 'easy_money_0': 143.12875283446712, 'easy_money_2': 180.43360544217688, 'easy_money_1': 104.10834467120182, 'cable_spool_fort_4': 123.26390022675737, 'cable_spool_fort_5': 54.69605442176871, 'cable_spool_fort_3': 105.82820861678005, 'cable_spool_fort_2': 137.65614512471655, 'cable_spool_fort_1': 134.8726984126984, 'cable_spool_fort_0': 100.59777777777778, 'the_black_willow_9': 188.23668934240362, 'the_black_willow_8': 142.4142403628118, 'the_black_willow_7': 172.399410430839, 'the_black_willow_6': 128.21628117913832, 'the_black_willow_5': 81.94331065759637, 'the_black_willow_4': 110.45659863945578, 'the_black_willow_2': 125.23859410430839, 'the_black_willow_3': 131.8818140589569, 'the_black_willow_10': 189.4689342403628, 'the_black_willow_11': 119.1855328798186, 'the_black_willow_1': 149.3205442176871, 'the_black_willow_0': 135.09065759637187}\n",
      "WAVE FILES WITH\\ NUMBERS:  {'lw1': 0.0, 'cable_spool_fort': 1.0, 'easy_money': 2.0, 'the_black_willow': 3.0}\n"
     ]
    }
   ],
   "source": [
    "stimuli_path = meg_path + '/stimuli/audio'\n",
    "wav_files_duration = {}\n",
    "\n",
    "for filename in os.listdir(stimuli_path):\n",
    "    if filename.endswith('.wav'): \n",
    "        file_path = os.path.join(stimuli_path, filename)\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        wav_files_duration[filename.rstrip('.wav')] = duration\n",
    "print('WAVE FILES DURATION: ',wav_files_duration)\n",
    "print('WAVE FILES WITH\\ NUMBERS: ',task)\n",
    "wav_list_without_numb = list(task.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION_MEG_TENSOR_TRAIN:  torch.Size([11958, 208, 16, 26])\n",
      "DIMENSION_MEG_TENSOR_VALID:  torch.Size([1684, 208, 16, 26])\n",
      "DIMENSION_MEG_TENSOR_TEST:  torch.Size([3480, 208, 16, 26])\n"
     ]
    }
   ],
   "source": [
    "megsp_path = os.path.join(meg_path, 'collect_data/megsp')\n",
    "megsp_list = os.listdir(megsp_path)\n",
    "\n",
    "megsp_list_session_0 = [f for f in megsp_list if f.split('_')[1] == '0']\n",
    "megsp_list_session_1 = [f for f in megsp_list if f.split('_')[1] == '1']\n",
    "\n",
    "meg_0_tensor_train, meg_0_tensor_valid, meg_0_tensor_test = get_splitted_tensor(megsp_list_session_0, megsp_path)\n",
    "meg_1_tensor_train, meg_1_tensor_valid, meg_1_tensor_test = get_splitted_tensor(megsp_list_session_1, megsp_path)\n",
    "meg_tensor_train = torch.cat((meg_0_tensor_train, meg_1_tensor_train), 0)\n",
    "meg_tensor_valid = torch.cat((meg_0_tensor_valid, meg_1_tensor_valid), 0)\n",
    "meg_tensor_test = torch.cat((meg_0_tensor_test, meg_1_tensor_test), 0)\n",
    "print('DIMENSION_MEG_TENSOR_TRAIN: ', meg_tensor_train.shape)\n",
    "print('DIMENSION_MEG_TENSOR_VALID: ', meg_tensor_valid.shape)\n",
    "print('DIMENSION_MEG_TENSOR_TEST: ', meg_tensor_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_mfcc = torch.load(os.path.join(meg_path, 'collect_data/audio_mfcc.pt'))\n",
    "audio_mel = torch.load(os.path.join(meg_path, 'collect_data/audio_mel.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION_AUDIO_TENSOR_TRAIN:  torch.Size([11958, 128, 376])\n",
      "DIMENSION_AUDIO_TENSOR_VALID:  torch.Size([1684, 128, 376])\n",
      "DIMENSION_AUDIO_TENSOR_TEST:  torch.Size([3480, 128, 376])\n"
     ]
    }
   ],
   "source": [
    "tensor_list_train = []\n",
    "tensor_list_valid = []\n",
    "tensor_list_test = []\n",
    "for file_tensor in audio_mel:\n",
    "    train_tensor, val_tensor, test_tensor = split_tensor(file_tensor)\n",
    "    tensor_list_train.append(train_tensor)\n",
    "    tensor_list_valid.append(val_tensor)\n",
    "    tensor_list_test.append(test_tensor)\n",
    "audio_tensor_train = torch.cat(tensor_list_train, dim=0)\n",
    "audio_tensor_valid = torch.cat(tensor_list_valid, dim=0)\n",
    "audio_tensor_test = torch.cat(tensor_list_test, dim=0)\n",
    "\n",
    "audio_tensor_train = torch.cat((audio_tensor_train, audio_tensor_train), 0)\n",
    "audio_tensor_valid = torch.cat((audio_tensor_valid, audio_tensor_valid), 0)\n",
    "audio_tensor_test = torch.cat((audio_tensor_test, audio_tensor_test), 0)\n",
    "print('DIMENSION_AUDIO_TENSOR_TRAIN: ', audio_tensor_train.shape)\n",
    "print('DIMENSION_AUDIO_TENSOR_VALID: ', audio_tensor_valid.shape)\n",
    "print('DIMENSION_AUDIO_TENSOR_TEST: ', audio_tensor_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [2:45:56<00:00, 47.87s/it]  \n"
     ]
    }
   ],
   "source": [
    "pred_target = []\n",
    "mse_scores = []\n",
    "real_target = []\n",
    "audio_train = audio_tensor_train.reshape(audio_tensor_train.shape[0], -1)\n",
    "audio_test = audio_tensor_test.reshape(audio_tensor_test.shape[0], -1)\n",
    "\n",
    "for channel in tqdm(range(num_channel)):   \n",
    "    y_train = meg_tensor_train[:, channel, :, :].reshape(meg_tensor_train.shape[0], -1)\n",
    "    y_test = meg_tensor_test[:, channel, :, :].reshape(meg_tensor_test.shape[0], -1)\n",
    "\n",
    "    model = Ridge(alpha=5000, max_iter=1000)\n",
    "    model.fit(audio_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(audio_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    pred_target.append(y_pred)\n",
    "    real_target.append(y_test)\n",
    "    mse_scores.append(mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_889212/3235489375.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /croot/pytorch_1686931851744/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  torch.save(torch.tensor(pred_target), save_pred_target)\n"
     ]
    }
   ],
   "source": [
    "save_pred_target = os.path.join(meg_path, 'collect_data/meg_prediction_ridge_mel.pt')\n",
    "torch.save(torch.tensor(pred_target), save_pred_target)\n",
    "save_mse = os.path.join(meg_path, 'collect_data/meg_mse_ridge_mel.pt')\n",
    "torch.save(torch.tensor(mse_scores), save_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-meg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
