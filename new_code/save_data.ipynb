{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30b07945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from param_funct import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from transformers import AutoProcessor, Wav2Vec2Model, ClapTextModelWithProjection\n",
    "from transformers import GPT2Tokenizer, GPT2Model, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84a346ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "model_text = GPT2Model.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "clap_text = ClapTextModelWithProjection.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "clap_tokenizer = AutoTokenizer.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "clap_model = clap_text.to(device)\n",
    "model_text = model_text.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcef5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli_path = meg_path + '/stimuli/audio'\n",
    "patient = ['01']\n",
    "sub_decim = 10\n",
    "brain_signal_data = []\n",
    "audio_spect_data = []\n",
    "audio_w2v_data = []\n",
    "text_gpt_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad4b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATIENT:  01\n",
      "SESSION:  0\n",
      "AUDIO_NAME:  lw1\n",
      "STORY_UID:  0\n",
      "Reading 0 ... 395999  =      0.000 ...   395.999 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteoc/spect-to-meg/new_code/param_funct.py:61: RuntimeWarning: Unable to map the following column(s) to to MNE:\n",
      "task_order: \"[0, 1, 2, 3]\"\n",
      "n_sessions: 2\n",
      "mri: fsaverage\n",
      "native_english_speaker: y\n",
      "  raw = mne_bids.read_raw_bids(bids_path, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 6601 samples (6.601 s)\n",
      "\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "SOUND_ID:  0.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([180, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([180, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([180, 99, 768])\n",
      "SOUND_ID:  1.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([139, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([139, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([139, 99, 768])\n",
      "SOUND_ID:  2.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([225, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([225, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([225, 99, 768])\n",
      "SOUND_ID:  3.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([124, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([124, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([124, 99, 768])\n",
      "AUDIO_NAME:  cable_spool_fort\n",
      "STORY_UID:  1\n",
      "Reading 0 ... 715999  =      0.000 ...   715.999 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteoc/spect-to-meg/new_code/param_funct.py:61: RuntimeWarning: Unable to map the following column(s) to to MNE:\n",
      "task_order: \"[0, 1, 2, 3]\"\n",
      "n_sessions: 2\n",
      "mri: fsaverage\n",
      "native_english_speaker: y\n",
      "  raw = mne_bids.read_raw_bids(bids_path, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 6601 samples (6.601 s)\n",
      "\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "SOUND_ID:  0.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([300, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([300, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([300, 99, 768])\n",
      "SOUND_ID:  1.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([325, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([325, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([325, 99, 768])\n",
      "SOUND_ID:  2.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([231, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([231, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([231, 99, 768])\n",
      "SOUND_ID:  3.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([249, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([249, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([249, 99, 768])\n",
      "SOUND_ID:  4.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([282, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([282, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([282, 99, 768])\n",
      "SOUND_ID:  5.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([116, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([116, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([116, 99, 768])\n",
      "AUDIO_NAME:  easy_money\n",
      "STORY_UID:  2\n",
      "Reading 0 ... 1191999  =      0.000 ...  1191.999 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteoc/spect-to-meg/new_code/param_funct.py:61: RuntimeWarning: Unable to map the following column(s) to to MNE:\n",
      "task_order: \"[0, 1, 2, 3]\"\n",
      "n_sessions: 2\n",
      "mri: fsaverage\n",
      "native_english_speaker: y\n",
      "  raw = mne_bids.read_raw_bids(bids_path, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 6601 samples (6.601 s)\n",
      "\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "SOUND_ID:  0.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([257, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([257, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([257, 99, 768])\n",
      "SOUND_ID:  1.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([190, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([190, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([190, 99, 768])\n",
      "SOUND_ID:  2.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([428, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([428, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([428, 99, 768])\n",
      "SOUND_ID:  3.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([378, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([378, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([378, 99, 768])\n",
      "SOUND_ID:  4.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([319, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([319, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([319, 99, 768])\n",
      "SOUND_ID:  5.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([410, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([410, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([410, 99, 768])\n",
      "SOUND_ID:  6.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([323, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([323, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([323, 99, 768])\n",
      "SOUND_ID:  7.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([332, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([332, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([332, 99, 768])\n",
      "AUDIO_NAME:  the_black_willow\n",
      "STORY_UID:  3\n",
      "Reading 0 ... 1868999  =      0.000 ...  1868.999 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteoc/spect-to-meg/new_code/param_funct.py:61: RuntimeWarning: Unable to map the following column(s) to to MNE:\n",
      "task_order: \"[0, 1, 2, 3]\"\n",
      "n_sessions: 2\n",
      "mri: fsaverage\n",
      "native_english_speaker: y\n",
      "  raw = mne_bids.read_raw_bids(bids_path, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 6601 samples (6.601 s)\n",
      "\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "SOUND_ID:  0.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([358, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([358, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([358, 99, 768])\n",
      "SOUND_ID:  1.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([262, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([262, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([262, 99, 768])\n",
      "SOUND_ID:  2.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([286, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([286, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([286, 99, 768])\n",
      "SOUND_ID:  3.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([306, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([306, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([306, 99, 768])\n",
      "SOUND_ID:  4.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([247, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([247, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([247, 99, 768])\n",
      "SOUND_ID:  5.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([185, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([185, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([185, 99, 768])\n",
      "SOUND_ID:  6.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([256, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([256, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([256, 99, 768])\n",
      "SOUND_ID:  7.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([384, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([384, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([384, 99, 768])\n",
      "SOUND_ID:  8.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([309, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([309, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([309, 99, 768])\n",
      "SOUND_ID:  9.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([411, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([411, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([411, 99, 768])\n",
      "SOUND_ID:  10.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([411, 208, 401])\n",
      "AUDIO_SHAPE:  torch.Size([411, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([411, 99, 768])\n",
      "SOUND_ID:  11.0\n",
      "Applying baseline correction (mode: mean)\n",
      "MEG_SHAPE:  torch.Size([338, 208, 401])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [23:13<00:00, 1393.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUDIO_SHAPE:  torch.Size([338, 257, 126])\n",
      "AUDIO_W2V:  torch.Size([338, 99, 768])\n",
      "SESSION:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for subject in tqdm(patient):\n",
    "    print('PATIENT: ', subject)\n",
    "    for sess in range(len(session)):\n",
    "        print(\"SESSION: \", session[sess])\n",
    "        if sess == 0:\n",
    "            for story in task_list:\n",
    "                print('AUDIO_NAME: ', story)\n",
    "                selected_sound_ids = tasks_with_sound_ids[story]\n",
    "                story_uid = int(task[story])\n",
    "                print(\"STORY_UID: \", story_uid)\n",
    "                raw = get_bids_raw(meg_path, subject, session[sess], str(story_uid))\n",
    "                for z, sound_id in enumerate(selected_sound_ids):\n",
    "                    print(\"SOUND_ID: \", float(sound_id))\n",
    "                    epochs_data = get_epochs(raw, float(story_uid), float(sound_id), sub_decim)\n",
    "                    epoch_signal = get_meg_from_raw_epochs(epochs_data)\n",
    "                    print('MEG_SHAPE: ', epoch_signal.shape)\n",
    "                    brain_signal_data.append(epoch_signal)\n",
    "                    \n",
    "                    if subject == '01':\n",
    "                        audio_path = f\"{stimuli_path}/{story}_{z}.wav\"\n",
    "                        waveform, sr = torchaudio.load(audio_path)\n",
    "                        if sr != sampling_audio:\n",
    "                            waveform = torchaudio.functional.resample(waveform, sr, sampling_audio)\n",
    "                        waveform = waveform.squeeze(0).to(device)\n",
    "                        data_audio_chunks = []\n",
    "                        data_audio_spect = []\n",
    "                        for j in range(epoch_signal.shape[0]):\n",
    "                            start = epochs_data[j]._metadata[\"start\"].item()\n",
    "                            sample_start = round(start * sampling_audio)\n",
    "                            sample_end = round((start + duration) * sampling_audio)\n",
    "                            y = waveform[sample_start:sample_end]\n",
    "                            expected_len = int(duration * sampling_audio)\n",
    "                            if y.shape[0] < expected_len:\n",
    "                                pad_len = expected_len - y.shape[0]\n",
    "                                y = torch.nn.functional.pad(y, (0, pad_len), value=0.0)\n",
    "                            elif y.shape[0] > expected_len:\n",
    "                                y = y[:expected_len]\n",
    "                            data_audio_chunks.append(y)\n",
    "                            spec = torchaudio.transforms.Spectrogram(n_fft=n_fft_speech, hop_length=hop_len_speech).to(device)(y.unsqueeze(0))\n",
    "                            spec_db = torchaudio.transforms.AmplitudeToDB()(spec)\n",
    "                            data_audio_spect.append(spec_db.squeeze(0))\n",
    "                        audio_tensor_chunk = torch.stack(data_audio_chunks)  # [batch, T]\n",
    "                        audio_tensor_spect = torch.stack(data_audio_spect)   \n",
    "                        inputs_w2v = processor(audio_tensor_chunk, sampling_rate=sampling_audio, return_tensors=\"pt\", padding=True)\n",
    "                        w2v_input = inputs_w2v.input_values.squeeze(0).to(device)\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(w2v_input)\n",
    "                        last_hidden_w2v = outputs.last_hidden_state.cpu()\n",
    "\n",
    "                        print('AUDIO_SHAPE: ', audio_tensor_spect.shape)\n",
    "                        print('AUDIO_W2V: ', last_hidden_w2v.shape)\n",
    "                        audio_spect_data.append(audio_tensor_spect)\n",
    "                        audio_w2v_data.append(last_hidden_w2v)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "brain_signal_data_tensor = torch.cat(brain_signal_data, dim=0)\n",
    "audio_spect_data_tensor = torch.cat(audio_spect_data, dim=0)\n",
    "audio_w2v_data_tensor = torch.cat(audio_w2v_data, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d102ca4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8561, 208, 401]),\n",
       " torch.Size([8561, 257, 126]),\n",
       " torch.Size([8561, 99, 768]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_signal_data_tensor.shape, audio_spect_data_tensor.shape, audio_w2v_data_tensor.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d068894",
   "metadata": {},
   "source": [
    "## TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a16e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in tqdm(patient):\n",
    "    epochs_list = []\n",
    "    for sess in tqdm(session):\n",
    "        print('---------', str(sess), '----------')\n",
    "        for task in [0, 1, 2, 3]:\n",
    "            print('---------', str(task), '----------')\n",
    "            bids_path = mne_bids.BIDSPath(\n",
    "                subject=subject,\n",
    "                session=str(sess),\n",
    "                task=str(task),\n",
    "                datatype=\"meg\",\n",
    "                root=meg_path,\n",
    "            )\n",
    "            try:\n",
    "                raw = mne_bids.read_raw_bids(bids_path, verbose=False)\n",
    "            except FileNotFoundError:\n",
    "                print(\"missing\", subject, sess, task)\n",
    "                pass\n",
    "            raw = raw.pick_types(\n",
    "                meg=True, misc=False, eeg=False, eog=False, ecg=False\n",
    "            )\n",
    "            raw.load_data().filter(0.5, 30.0, n_jobs=1)\n",
    "            if task == 0:\n",
    "                for sound_id in lw1:\n",
    "                    epochs = get_epochs(raw, float(task), float(sound_id), sub_decim)\n",
    "                    epochs_list.append(epochs)\n",
    "            if task == 1:\n",
    "                for sound_id in cable_spool_fort:\n",
    "                    epochs = get_epochs(raw, float(task), float(sound_id), sub_decim)\n",
    "                    epochs_list.append(epochs)\n",
    "            if task == 2:\n",
    "                for sound_id in easy_money:\n",
    "                    epochs = get_epochs(raw, float(task), float(sound_id), sub_decim)\n",
    "                    epochs_list.append(epochs)\n",
    "            if task == 3:\n",
    "                for sound_id in the_black_willow:\n",
    "                    epochs = get_epochs(raw, float(task), float(sound_id), sub_decim)\n",
    "                    epochs_list.append(epochs)\n",
    "\n",
    "    if subject == '01':\n",
    "        concat_epochs = mne.concatenate_epochs(epochs_list)\n",
    "        y_text = concat_epochs.metadata.word.to_numpy()\n",
    "        X_brain = concat_epochs.get_data()\n",
    "        _, y_text_sentence = generate_sent_matrix(X_brain, y_text)\n",
    "\n",
    "        # tokenizer.pad_token = tokenizer.eos_token\n",
    "        inputs_tr = clap_tokenizer(list(y_text_sentence), padding=True, return_tensors=\"pt\")\n",
    "        clap_model.eval().to('cpu')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs_tr = clap_model(**inputs_tr)\n",
    "        last_hidden_states_tr = outputs_tr.last_hidden_state.cpu()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "764d4ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_tr = clap_tokenizer(list(y_text_sentence), padding=True, return_tensors=\"pt\")\n",
    "clap_model.eval().to('cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "        outputs_tr = clap_model(**inputs_tr)\n",
    "last_hidden_states_tr = outputs_tr.text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aead5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2: torch.Size([17122, 35, 768])\n",
    "# CLAP Last: torch.Size([17122, 35, 768])\n",
    "# CLAP Feat: torch.Size([17122, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c957e9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17122, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0dfb78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "meg_path = '/srv/nfs-data/sisko/matteoc/meg'\n",
    "save_stimulus_dir = os.path.join(meg_path, 'save_stimulus')\n",
    "\n",
    "torch.save(last_hidden_states_tr, os.path.join(save_stimulus_dir, \"clap_512_tensor.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
